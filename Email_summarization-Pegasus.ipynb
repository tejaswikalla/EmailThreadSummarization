{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49054fa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 254\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/vkalla/.local/lib/python3.9/site-packages (1.7.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: xxhash in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/vkalla/.local/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (3.11.6)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.49.0\n",
      "    Uninstalling tqdm-4.49.0:\n",
      "      Successfully uninstalled tqdm-4.49.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.12\n",
      "    Uninstalling huggingface-hub-0.0.12:\n",
      "      Successfully uninstalled huggingface-hub-0.0.12\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.7.0\n",
      "    Uninstalling datasets-1.7.0:\n",
      "      Successfully uninstalled datasets-1.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai 1.55.0 requires pydantic<3,>=1.9.0, but you have pydantic 1.7.4 which is incompatible.\n",
      "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
      "questeval 0.2.4 requires datasets==1.7.0, but you have datasets 3.1.0 which is incompatible.\n",
      "transformers 4.8.1 requires huggingface-hub==0.0.12, but you have huggingface-hub 0.26.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 huggingface-hub-0.26.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c76f395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.8.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading tokenizers-0.20.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/vkalla/.local/lib/python3.9/site-packages/~-kenizers'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.8.1\n",
      "    Uninstalling transformers-4.8.1:\n",
      "      Successfully uninstalled transformers-4.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
      "questeval 0.2.4 requires datasets==1.7.0, but you have datasets 3.1.0 which is incompatible.\n",
      "questeval 0.2.4 requires transformers==4.8.1, but you have transformers 4.46.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4792430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EmailParquetPreprocessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the email preprocessor for parquet files\"\"\"\n",
    "        self.df = None\n",
    "        self.processed_threads = []\n",
    "\n",
    "    def load_parquet(self, file_path: str) -> None:\n",
    "        \"\"\"Load email threads from a parquet file.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_parquet(file_path)\n",
    "            print(f\"Loaded {len(self.df)} email threads from parquet file\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading parquet file: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_body(body: str) -> str:\n",
    "        \"\"\"Clean and format the email body text.\"\"\"\n",
    "        if not isinstance(body, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Remove quoted messages and header details\n",
    "        body = re.sub(r'-{3,}Original Message-{3,}.*?(?=\\n\\n|\\Z)', '', body, flags=re.DOTALL)\n",
    "        body = re.sub(r'(From|Sent|To|Subject):.*?\\n', '', body)\n",
    "\n",
    "        # Handle encoding artifacts and whitespace\n",
    "        body = re.sub(r'=\\s*\\n', '', body)   # Remove soft line breaks\n",
    "        body = re.sub(r'=\\d{2}', '', body)  \n",
    "        return re.sub(r'\\s+', ' ', body).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sender_name(from_field: str) -> str:\n",
    "        \"\"\"Extract sender's name, reformatting 'Last, First' to 'First Last'.\"\"\"\n",
    "        if not isinstance(from_field, str):\n",
    "            return \"\"\n",
    "\n",
    "        match = re.match(r'([^<]+)', from_field)\n",
    "        if match:\n",
    "            name = match.group(1).strip()\n",
    "            parts = name.split(',')\n",
    "            return f\"{parts[1].strip()} {parts[0].strip()}\" if len(parts) == 2 else name\n",
    "        return from_field\n",
    "\n",
    "    def process_single_thread(self, thread_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single email thread into a structured format.\"\"\"\n",
    "        structured_thread = {\n",
    "            'subject': thread_data.get('subject', '').strip(),\n",
    "            'messages': []\n",
    "        }\n",
    "\n",
    "        for msg in thread_data.get('messages', []):\n",
    "            # Default to an empty string if the timestamp is missing or not a string\n",
    "            timestamp = msg.get('timestamp', '')\n",
    "            if isinstance(timestamp, datetime):\n",
    "                try:\n",
    "                    formatted_timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    # If timestamp is not in ISO format, skip this message\n",
    "                    formatted_timestamp = 'Invalid Timestamp'\n",
    "            else:\n",
    "                formatted_timestamp = 'Invalid Timestamp'\n",
    "\n",
    "            cleaned_msg = {\n",
    "                'timestamp': formatted_timestamp,\n",
    "                'sender': self.extract_sender_name(msg.get('from', '')),\n",
    "                'recipients': [self.extract_sender_name(recipient) for recipient in msg.get('to', [])],\n",
    "                'body': self.clean_body(msg.get('body', ''))\n",
    "            }\n",
    "            structured_thread['messages'].append(cleaned_msg)\n",
    "\n",
    "        # Sort messages by timestamp (excluding any with 'Invalid Timestamp' if desired)\n",
    "        structured_thread['messages'].sort(key=lambda x: x['timestamp'] if x['timestamp'] != 'Invalid Timestamp' else '9999-12-31 23:59:59')\n",
    "\n",
    "        # Create summary input format\n",
    "        structured_thread['summary_input'] = self.format_for_summary(structured_thread)     \n",
    "        return structured_thread\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def format_for_summary(thread: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format structured thread data into a summarization-ready format.\"\"\"\n",
    "        summary_input = f\"Subject: {thread['subject']}\\n\\n\"\n",
    "\n",
    "        for msg in thread['messages']:\n",
    "            summary_input += f\"[{msg['timestamp']}] {msg['sender']}:\\n{msg['body']}\\n\\n\"\n",
    "\n",
    "        return summary_input.strip()\n",
    "\n",
    "    def process_all_threads(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process all email threads loaded from the parquet file.\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"No parquet file loaded. Call load_parquet() first.\")\n",
    "\n",
    "        self.processed_threads = []\n",
    "\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Processing threads\"):\n",
    "            try:\n",
    "                thread_data = json.loads(row['thread']) if isinstance(row['thread'], str) else row['thread']\n",
    "                \n",
    "                processed_thread = self.process_single_thread(thread_data)\n",
    "                processed_thread['summary'] = row.get('summary', '') \n",
    "                \n",
    "                self.processed_threads.append(processed_thread)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing thread at index {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return self.processed_threads\n",
    "\n",
    "    def save_processed_threads(self, output_path: str) -> None:\n",
    "        \"\"\"Save processed threads to a new parquet file.\"\"\"\n",
    "        if not self.processed_threads:\n",
    "            raise ValueError(\"No processed threads to save. Run process_all_threads() first.\")\n",
    "\n",
    "        processed_df = pd.DataFrame([{\n",
    "            'subject': thread['subject'],\n",
    "            'summary_input': thread['summary_input'],\n",
    "            'processed_messages': json.dumps(thread['messages']),\n",
    "            'summary' : thread['summary']\n",
    "        } for thread in self.processed_threads])\n",
    "\n",
    "        processed_df.to_parquet(output_path, index=False)\n",
    "        print(f\"Saved {len(processed_df)} processed threads to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce25e892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3750 email threads from parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing threads: 100%|██████████| 3750/3750 [00:02<00:00, 1867.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3750 processed threads to processed_threads.parquet\n",
      "\n",
      "Sample processed thread:\n",
      "Subject: FW: Master Termination Log\n",
      "Number of messages: 5\n",
      "\n",
      "Sample summary input:\n",
      "Subject: FW: Master Termination Log\n",
      "\n",
      "[2002-01-29 11:23:42] Jeffrey C. Gossett:\n",
      "Attached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan...\n",
      "{'subject': 'FW: Master Termination Log', 'messages': [{'timestamp': '2002-01-29 11:23:42', 'sender': 'Jeffrey C. Gossett', 'recipients': ['Giron', 'Darron C.', 'Love', 'Phillip M.'], 'body': 'Attached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan CountyEPMIdeal no. 757497.01 Connect Energy Services, Inc.ENAliquids agreement NGL Supply, Inc. (including PremierENA/EGLIphysical & financial transactions referenced Energy Partners, a division of NGL Supply, Inc.) Plains Marketing, L.P.ERACdeal no. QG4563.1 Plains Marketing, L.P.ERACdeal no. QG4482.2 Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-01-31 12:50:00', 'sender': 'Kim S. Theriot', 'recipients': ['Murphy', 'Melissa', 'Gossett', 'Jeffrey C.', 'White', 'Stacey W.', 'Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Anderson', 'Diane', 'Hunter', 'Larry Joe'], 'body': 'Attached are the Daily Lists for January 29 and January 30 as well as the Master Termination Log, which incorporates all terminations received through January 30. Also, prepetition mutual terminations have been added to this list. They are identified under \"Nature of Default\" as \"mutual termination\". Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-02-05 15:03:35', 'sender': 'Kim S. Theriot', 'recipients': ['Murphy', 'Melissa', 'Anderson', 'Diane', 'White', 'Stacey W.', 'Gossett', 'Jeffrey C.', 'Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Aucoin', 'Evelyn', 'Baxter', 'Bryce'], 'body': \"Note to Stephanie Panus.... Stephanie...please remove my name as well as Melissa Murphy's from the distribution list below. Please add the following: Todd D. Hall Kevin Sweeney Rita Wynne Rebecca Grace Rhonda Robinson Kerri Thomspon Kristin Albrecht Tom Chapman Thanks! Kim Theriot Attached is the Daily List for January 31 as well as the Master Termination Log, which incorporates all terminations received through January 31. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\"}, {'timestamp': '2002-02-05 15:06:25', 'sender': 'Kim S. Theriot', 'recipients': ['Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Anderson', 'Diane', 'Gossett', 'Jeffrey C.', 'White', 'Stacey W.', 'Murphy', 'Melissa'], 'body': 'Attached is the Daily List for February 4 as well as the Master Termination Log, which incorporates all termination received through February 4 (with the exception of February 1, which is under legal review and contains all financial transactions). Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-05-28 07:20:35', 'sender': 'Katherine L. Kelly', 'recipients': ['Germany', 'Chris'], 'body': 'Please look into the CNG LDC (Hope Gas) termination 12/1 and the $66 MM settlement offer that is listed on the Letter Log below. Let me know what that is after you figure it out. If you have any questions, please ask. Ed Attached is the Daily List for May 24, 2002 as well as the Master Termination Log, which incorporates all terminations received through May 24. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}], 'summary_input': 'Subject: FW: Master Termination Log\\n\\n[2002-01-29 11:23:42] Jeffrey C. Gossett:\\nAttached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan CountyEPMIdeal no. 757497.01 Connect Energy Services, Inc.ENAliquids agreement NGL Supply, Inc. (including PremierENA/EGLIphysical & financial transactions referenced Energy Partners, a division of NGL Supply, Inc.) Plains Marketing, L.P.ERACdeal no. QG4563.1 Plains Marketing, L.P.ERACdeal no. QG4482.2 Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-01-31 12:50:00] Kim S. Theriot:\\nAttached are the Daily Lists for January 29 and January 30 as well as the Master Termination Log, which incorporates all terminations received through January 30. Also, prepetition mutual terminations have been added to this list. They are identified under \"Nature of Default\" as \"mutual termination\". Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-02-05 15:03:35] Kim S. Theriot:\\nNote to Stephanie Panus.... Stephanie...please remove my name as well as Melissa Murphy\\'s from the distribution list below. Please add the following: Todd D. Hall Kevin Sweeney Rita Wynne Rebecca Grace Rhonda Robinson Kerri Thomspon Kristin Albrecht Tom Chapman Thanks! Kim Theriot Attached is the Daily List for January 31 as well as the Master Termination Log, which incorporates all terminations received through January 31. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-02-05 15:06:25] Kim S. Theriot:\\nAttached is the Daily List for February 4 as well as the Master Termination Log, which incorporates all termination received through February 4 (with the exception of February 1, which is under legal review and contains all financial transactions). Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-05-28 07:20:35] Katherine L. Kelly:\\nPlease look into the CNG LDC (Hope Gas) termination 12/1 and the $66 MM settlement offer that is listed on the Letter Log below. Let me know what that is after you figure it out. If you have any questions, please ask. Ed Attached is the Daily List for May 24, 2002 as well as the Master Termination Log, which incorporates all terminations received through May 24. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490', 'summary': \"The email thread discusses the Master Termination Log and the need to investigate a CNG LDC (Hope Gas) termination and a $66 million settlement offer. Stephanie Panus sends out the Daily List and Master Termination Log for various dates. Kim Theriot requests her name and Melissa Murphy's name to be removed from the distribution list and adds several names to it. The thread also includes updates on terminations and valid terminations for various companies.\"}\n"
     ]
    }
   ],
   "source": [
    "preprocessor = EmailParquetPreprocessor()\n",
    "\n",
    "# Load parquet file\n",
    "preprocessor.load_parquet('train-00000-of-00001-41f2ca6bce8b68f8.parquet')\n",
    "\n",
    "# Process all threads\n",
    "processed_threads = preprocessor.process_all_threads()\n",
    "\n",
    "# Save processed results\n",
    "preprocessor.save_processed_threads('processed_threads.parquet')\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample processed thread:\")\n",
    "sample_thread = processed_threads[0]\n",
    "print(f\"Subject: {sample_thread['subject']}\")\n",
    "print(f\"Number of messages: {len(sample_thread['messages'])}\")\n",
    "print(\"\\nSample summary input:\")\n",
    "print(sample_thread['summary_input'][:500] + \"...\")\n",
    "print(sample_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f48c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    MAX_INPUT_LENGTH = 512\n",
    "    OUTPUT_DIR = './pegasus_results'\n",
    "    SAVED_DIR = './saved_pegasus_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf67f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90946c92c08d4434921eaa687924cc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=63):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebcf13b78e74634a231239ecd369aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=63):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 14:14:00,032] A new study created in memory with name: no-name-834d681b-3919-4455-9bea-96cc9aac78b7\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1683' max='1683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1683/1683 1:28:53, Epoch 8/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.401836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.359848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.350874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.348124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.347474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkalla/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 15:44:07,321] Trial 0 finished with value: 0.3474743366241455 and parameters: {'learning_rate': 7.293807020845046e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [465/465 47:31, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>0.350169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.347738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.352805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.359742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 16:32:26,289] Trial 1 finished with value: 0.3477378785610199 and parameters: {'learning_rate': 0.0003574032313977492, 'num_train_epochs': 5, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1500 45:05 < 22:35, 0.37 it/s, Epoch 4/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.349033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.350430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.352051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.352687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 17:18:14,700] Trial 2 finished with value: 0.3490328788757324 and parameters: {'learning_rate': 3.554967218658174e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 6}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='1496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/1496 39:32 < 39:26, 0.32 it/s, Epoch 4/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.370013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>0.376208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.390606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 17:58:30,555] Trial 3 finished with value: 0.37001264095306396 and parameters: {'learning_rate': 0.00026902299270575375, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='642' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [642/856 57:34 < 19:15, 0.19 it/s, Epoch 5/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.427181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.390509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.383052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.389141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.398907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.405254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 18:56:54,153] Trial 4 finished with value: 0.38305187225341797 and parameters: {'learning_rate': 0.00019634610560278755, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 7}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 34:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.381291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.399279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.417668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 19:32:01,592] Trial 5 finished with value: 0.3812909424304962 and parameters: {'learning_rate': 0.00024100187605160443, 'num_train_epochs': 3, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 5}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 56:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.480528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.411397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.417872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.418314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.421493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 20:29:18,921] Trial 6 finished with value: 0.4113965332508087 and parameters: {'learning_rate': 4.776761237143552e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 6}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/3000 1:08:15 < 45:33, 0.44 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.557440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.446429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.442688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.446483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.472627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.482697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 21:38:16,286] Trial 7 finished with value: 0.4426884353160858 and parameters: {'learning_rate': 0.00014126347053151992, 'num_train_epochs': 10, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 5}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 875/1000 1:07:21 < 09:38, 0.22 it/s, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.559635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.534447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.487157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.495355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.502011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.504904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 22:46:21,917] Trial 8 finished with value: 0.4871566593647003 and parameters: {'learning_rate': 8.625851358918672e-05, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 6}. Best is trial 0 with value: 0.3474743366241455.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 57:48, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.633065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.695760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.634952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.586006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.547559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.561485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 23:44:54,962] Trial 9 finished with value: 0.5475591421127319 and parameters: {'learning_rate': 0.00027850828310803936, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 6}. Best is trial 0 with value: 0.3474743366241455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'learning_rate': 7.293807020845046e-05, 'num_train_epochs': 9, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_pegasus_model/tokenizer_config.json',\n",
       " './saved_pegasus_model/special_tokens_map.json',\n",
       " './saved_pegasus_model/spiece.model',\n",
       " './saved_pegasus_model/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments, Adafactor, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    peg_df = pd.read_parquet(file_path)\n",
    "    peg_df = peg_df.dropna(subset=['summary_input', 'summary'])\n",
    "    train_df, val_df = train_test_split(\n",
    "        peg_df,\n",
    "        test_size=0.2,\n",
    "        random_state=Config.RANDOM_SEED\n",
    "    )\n",
    "    return train_df, val_df\n",
    "\n",
    "def preprocess_function(tokenizer, threads):\n",
    "    model_inputs = tokenizer(\n",
    "        threads['summary_input'],\n",
    "        max_length=Config.MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['summary'],\n",
    "        max_length=Config.MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_df, val_df = load_and_preprocess_data('processed_threads.parquet')\n",
    "\n",
    "# Convert to datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['summary_input', 'summary']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['summary_input', 'summary']])\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_tokenized_dataset = train_dataset.map(lambda x: preprocess_function(tokenizer, x), batched=True, num_proc=max(2, os.cpu_count()-1))\n",
    "val_tokenized_dataset = val_dataset.map(lambda x: preprocess_function(tokenizer, x), batched=True, num_proc=max(2, os.cpu_count()-1))\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 10)\n",
    "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [2, 4])\n",
    "    gradient_accumulation_steps = trial.suggest_int('gradient_accumulation_steps', 4, 8)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        eval_strategy='epoch',\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=10,\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        report_to='tensorboard',\n",
    "        fp16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n",
    "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        dataloader_num_workers=10,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=1.0,\n",
    "        remove_unused_columns=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        skip_memory_metrics=True,\n",
    "        lr_scheduler_type='linear'\n",
    "    )\n",
    "\n",
    "    optimizer = Adafactor(model.parameters(), lr=learning_rate, scale_parameter=False, relative_step=False)\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        optimizers=(optimizer, None),\n",
    "        train_dataset=train_tokenized_dataset,\n",
    "        eval_dataset=val_tokenized_dataset,\n",
    "        data_collator=None,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result['eval_loss']\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial.params}\")\n",
    "model.save_pretrained(Config.SAVED_DIR)\n",
    "tokenizer.save_pretrained(Config.SAVED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0ec0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 417 email threads from parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing threads: 100%|██████████| 417/417 [00:00<00:00, 1914.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 417 processed threads to processed_test_threads.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test parquet file\n",
    "preprocessor.load_parquet('test-00000-of-00001-cd15b40aacd3c33e.parquet')\n",
    "\n",
    "# Process all threads\n",
    "processed_test_threads = preprocessor.process_all_threads()\n",
    "\n",
    "# Save processed results\n",
    "preprocessor.save_processed_threads('processed_test_threads.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df343218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [1:00:54<00:00,  8.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the processed data\n",
    "test_df = pd.read_parquet('processed_test_threads.parquet')\n",
    "\n",
    "# Check for NaN values and drop them\n",
    "test_df = test_df.dropna(subset=['summary_input'])\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = PegasusForConditionalGeneration.from_pretrained('./saved_pegasus_model')\n",
    "tokenizer = PegasusTokenizer.from_pretrained('./saved_pegasus_model')\n",
    "\n",
    "# Generate summaries for the test data\n",
    "def generate_summary(threads):\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            threads['summary_input'],\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        if input_ids.max() >= tokenizer.vocab_size:\n",
    "            print(f\"Out-of-range token ID detected. Max ID: {input_ids.max()}, Vocab size: {tokenizer.vocab_size}\")\n",
    "            return \"\"\n",
    "        summary_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=512,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing input: {example['subject']}, Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "tqdm.pandas()\n",
    "test_df['generated_summary'] = test_df.progress_apply(lambda x: generate_summary(x), axis=1)\n",
    "\n",
    "# Save only the summary_input and generated_summary columns\n",
    "test_df[['summary', 'generated_summary']].to_csv('pegasus_generated_test_summaries.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
