{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3ebdbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.1)\n",
      "Requirement already satisfied: dataset in /home/vkalla/.local/lib/python3.9/site-packages (1.6.2)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (4.63.1)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in /home/vkalla/.local/lib/python3.9/site-packages (from dataset) (1.4.54)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /home/vkalla/.local/lib/python3.9/site-packages (from dataset) (1.14.0)\n",
      "Requirement already satisfied: banal>=1.0.1 in /home/vkalla/.local/lib/python3.9/site-packages (from dataset) (1.0.6)\n",
      "Requirement already satisfied: Mako in /home/vkalla/.local/lib/python3.9/site-packages (from alembic>=0.6.2->dataset) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/vkalla/.local/lib/python3.9/site-packages (from alembic>=0.6.2->dataset) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/vkalla/.local/lib/python3.9/site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229b5e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (1.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (18.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (66 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
      "Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading propcache-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading yarl-1.17.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.21.0 which is incompatible.\n",
      "label-studio 1.4.1.post1 requires python-dateutil==2.8.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "label-studio 1.4.1.post1 requires pytz==2019.3, but you have pytz 2024.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.6 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 requests-2.32.3 tqdm-4.67.0 xxhash-3.5.0 yarl-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b01119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/vkalla/.local/lib/python3.9/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (13.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (61.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.24.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.6.0)\n",
      "Requirement already satisfied: namex in /home/vkalla/.local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/vkalla/.local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2021.10.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/vkalla/.local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging->tensorflow<2.19,>=2.18->tf-keras) (3.0.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (4.11.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Installing collected packages: numpy, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.0\n",
      "    Uninstalling numpy-1.21.0:\n",
      "      Successfully uninstalled numpy-1.21.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/vkalla/.local/lib/python3.9/site-packages/~umpy.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/vkalla/.local/lib/python3.9/site-packages/~umpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "label-studio 1.4.1.post1 requires python-dateutil==2.8.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "label-studio 1.4.1.post1 requires pytz==2019.3, but you have pytz 2024.2 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.2 tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d60952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/vkalla/.local/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.3.5)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Installing collected packages: pandas\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "label-studio 1.4.1.post1 requires python-dateutil==2.8.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "label-studio 1.4.1.post1 requires pytz==2019.3, but you have pytz 2024.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d1a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (4.63.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8fc7fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70aad9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/vkalla/.local/lib/python3.9/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf945d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce337cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d24842e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vkalla/.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de66e12c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in /home/vkalla/.local/lib/python3.9/site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (1.12.1+cu113)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/vkalla/.local/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/vkalla/.local/lib/python3.9/site-packages (from datasets) (3.11.6)\n",
      "Requirement already satisfied: typing-extensions in /home/vkalla/.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vkalla/.local/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vkalla/.local/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799058e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: nltk in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rouge-score) (3.6.7)\n",
      "Requirement already satisfied: numpy in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rouge-score) (1.22.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk->rouge-score) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk->rouge-score) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /home/vkalla/.local/lib/python3.9/site-packages (from nltk->rouge-score) (4.67.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=3c096280342d43d0dd3d3588e8bb4cf9af00bdb90328610ad15848f4bb519361\n",
      "  Stored in directory: /home/vkalla/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b49801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb692df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/vkalla/.local/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vkalla/.local/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vkalla/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e358c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/vkalla/.local/lib/python3.9/site-packages (from optuna) (1.14.0)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from optuna) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/vkalla/.local/lib/python3.9/site-packages (from optuna) (1.4.54)\n",
      "Requirement already satisfied: tqdm in /home/vkalla/.local/lib/python3.9/site-packages (from optuna) (4.67.0)\n",
      "Requirement already satisfied: PyYAML in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in /home/vkalla/.local/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/vkalla/.local/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/vkalla/.local/lib/python3.9/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.9.0 optuna-4.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee0ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m133.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7919cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b878837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dde6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EmailParquetPreprocessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the email preprocessor for parquet files\"\"\"\n",
    "        self.df = None\n",
    "        self.processed_threads = []\n",
    "\n",
    "    def load_parquet(self, file_path: str) -> None:\n",
    "        \"\"\"Load email threads from a parquet file.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_parquet(file_path)\n",
    "            print(f\"Loaded {len(self.df)} email threads from parquet file\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading parquet file: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_body(body: str) -> str:\n",
    "        \"\"\"Clean and format the email body text.\"\"\"\n",
    "        if not isinstance(body, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Remove quoted messages and header details\n",
    "        body = re.sub(r'-{3,}Original Message-{3,}.*?(?=\\n\\n|\\Z)', '', body, flags=re.DOTALL)\n",
    "        body = re.sub(r'(From|Sent|To|Subject):.*?\\n', '', body)\n",
    "\n",
    "        # Handle encoding artifacts and whitespace\n",
    "        body = re.sub(r'=\\s*\\n', '', body)   # Remove soft line breaks\n",
    "        body = re.sub(r'=\\d{2}', '', body)  \n",
    "        return re.sub(r'\\s+', ' ', body).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sender_name(from_field: str) -> str:\n",
    "        \"\"\"Extract sender's name, reformatting 'Last, First' to 'First Last'.\"\"\"\n",
    "        if not isinstance(from_field, str):\n",
    "            return \"\"\n",
    "\n",
    "        match = re.match(r'([^<]+)', from_field)\n",
    "        if match:\n",
    "            name = match.group(1).strip()\n",
    "            parts = name.split(',')\n",
    "            return f\"{parts[1].strip()} {parts[0].strip()}\" if len(parts) == 2 else name\n",
    "        return from_field\n",
    "\n",
    "    def process_single_thread(self, thread_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single email thread into a structured format.\"\"\"\n",
    "        structured_thread = {\n",
    "            'subject': thread_data.get('subject', '').strip(),\n",
    "            'messages': []\n",
    "        }\n",
    "\n",
    "        for msg in thread_data.get('messages', []):\n",
    "            # Default to an empty string if the timestamp is missing or not a string\n",
    "            timestamp = msg.get('timestamp', '')\n",
    "            if isinstance(timestamp, datetime):\n",
    "                try:\n",
    "                    formatted_timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    # If timestamp is not in ISO format, skip this message\n",
    "                    formatted_timestamp = 'Invalid Timestamp'\n",
    "            else:\n",
    "                formatted_timestamp = 'Invalid Timestamp'\n",
    "\n",
    "            cleaned_msg = {\n",
    "                'timestamp': formatted_timestamp,\n",
    "                'sender': self.extract_sender_name(msg.get('from', '')),\n",
    "                'recipients': [self.extract_sender_name(recipient) for recipient in msg.get('to', [])],\n",
    "                'body': self.clean_body(msg.get('body', ''))\n",
    "            }\n",
    "            structured_thread['messages'].append(cleaned_msg)\n",
    "\n",
    "        # Sort messages by timestamp (excluding any with 'Invalid Timestamp' if desired)\n",
    "        structured_thread['messages'].sort(key=lambda x: x['timestamp'] if x['timestamp'] != 'Invalid Timestamp' else '9999-12-31 23:59:59')\n",
    "\n",
    "        # Create summary input format\n",
    "        structured_thread['summary_input'] = self.format_for_summary(structured_thread)\n",
    "        \n",
    "        return structured_thread\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def format_for_summary(thread: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format structured thread data into a summarization-ready format.\"\"\"\n",
    "        summary_input = f\"Subject: {thread['subject']}\\n\\n\"\n",
    "\n",
    "        for msg in thread['messages']:\n",
    "            summary_input += f\"[{msg['timestamp']}] {msg['sender']}:\\n{msg['body']}\\n\\n\"\n",
    "\n",
    "        return summary_input.strip()\n",
    "\n",
    "    def process_all_threads(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process all email threads loaded from the parquet file.\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"No parquet file loaded. Call load_parquet() first.\")\n",
    "\n",
    "        self.processed_threads = []\n",
    "\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Processing threads\"):\n",
    "            try:\n",
    "                thread_data = json.loads(row['thread']) if isinstance(row['thread'], str) else row['thread']\n",
    "                \n",
    "                processed_thread = self.process_single_thread(thread_data)\n",
    "                processed_thread['summary'] = row.get('summary', '') \n",
    "                \n",
    "                self.processed_threads.append(processed_thread)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing thread at index {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return self.processed_threads\n",
    "\n",
    "    def save_processed_threads(self, output_path: str) -> None:\n",
    "        \"\"\"Save processed threads to a new parquet file.\"\"\"\n",
    "        if not self.processed_threads:\n",
    "            raise ValueError(\"No processed threads to save. Run process_all_threads() first.\")\n",
    "\n",
    "        processed_df = pd.DataFrame([{\n",
    "            'subject': thread['subject'],\n",
    "            'summary_input': thread['summary_input'],\n",
    "            'processed_messages': json.dumps(thread['messages']),\n",
    "            'summary' : thread['summary']\n",
    "        } for thread in self.processed_threads])\n",
    "\n",
    "        processed_df.to_parquet(output_path, index=False)\n",
    "        print(f\"Saved {len(processed_df)} processed threads to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25e892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3750 email threads from parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing threads: 100%|██████████| 3750/3750 [00:02<00:00, 1840.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3750 processed threads to processed_threads.parquet\n",
      "\n",
      "Sample processed thread:\n",
      "Subject: FW: Master Termination Log\n",
      "Number of messages: 5\n",
      "\n",
      "Sample summary input:\n",
      "Subject: FW: Master Termination Log\n",
      "\n",
      "[2002-01-29 11:23:42] Jeffrey C. Gossett:\n",
      "Attached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan...\n",
      "{'subject': 'FW: Master Termination Log', 'messages': [{'timestamp': '2002-01-29 11:23:42', 'sender': 'Jeffrey C. Gossett', 'recipients': ['Giron', 'Darron C.', 'Love', 'Phillip M.'], 'body': 'Attached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan CountyEPMIdeal no. 757497.01 Connect Energy Services, Inc.ENAliquids agreement NGL Supply, Inc. (including PremierENA/EGLIphysical & financial transactions referenced Energy Partners, a division of NGL Supply, Inc.) Plains Marketing, L.P.ERACdeal no. QG4563.1 Plains Marketing, L.P.ERACdeal no. QG4482.2 Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-01-31 12:50:00', 'sender': 'Kim S. Theriot', 'recipients': ['Murphy', 'Melissa', 'Gossett', 'Jeffrey C.', 'White', 'Stacey W.', 'Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Anderson', 'Diane', 'Hunter', 'Larry Joe'], 'body': 'Attached are the Daily Lists for January 29 and January 30 as well as the Master Termination Log, which incorporates all terminations received through January 30. Also, prepetition mutual terminations have been added to this list. They are identified under \"Nature of Default\" as \"mutual termination\". Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-02-05 15:03:35', 'sender': 'Kim S. Theriot', 'recipients': ['Murphy', 'Melissa', 'Anderson', 'Diane', 'White', 'Stacey W.', 'Gossett', 'Jeffrey C.', 'Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Aucoin', 'Evelyn', 'Baxter', 'Bryce'], 'body': \"Note to Stephanie Panus.... Stephanie...please remove my name as well as Melissa Murphy's from the distribution list below. Please add the following: Todd D. Hall Kevin Sweeney Rita Wynne Rebecca Grace Rhonda Robinson Kerri Thomspon Kristin Albrecht Tom Chapman Thanks! Kim Theriot Attached is the Daily List for January 31 as well as the Master Termination Log, which incorporates all terminations received through January 31. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\"}, {'timestamp': '2002-02-05 15:06:25', 'sender': 'Kim S. Theriot', 'recipients': ['Hall', 'D. Todd', 'Sweeney', 'Kevin', 'Anderson', 'Diane', 'Gossett', 'Jeffrey C.', 'White', 'Stacey W.', 'Murphy', 'Melissa'], 'body': 'Attached is the Daily List for February 4 as well as the Master Termination Log, which incorporates all termination received through February 4 (with the exception of February 1, which is under legal review and contains all financial transactions). Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}, {'timestamp': '2002-05-28 07:20:35', 'sender': 'Katherine L. Kelly', 'recipients': ['Germany', 'Chris'], 'body': 'Please look into the CNG LDC (Hope Gas) termination 12/1 and the $66 MM settlement offer that is listed on the Letter Log below. Let me know what that is after you figure it out. If you have any questions, please ask. Ed Attached is the Daily List for May 24, 2002 as well as the Master Termination Log, which incorporates all terminations received through May 24. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490'}], 'summary_input': 'Subject: FW: Master Termination Log\\n\\n[2002-01-29 11:23:42] Jeffrey C. Gossett:\\nAttached is the Daily Termination List for January 25 as well as the Master Termination Log, which incorporates all terminations received through January 25. The following were previously on the Master Termination Log and have now been marked as \"Y\" for a valid termination: Atlantic Coast Fibers, Inc.ENApulp/paper transactions CNC-Containers CorporationEPMImaster power agreement Public Utility District No. 1 of Chelan CountyEPMIdeal no. 757497.01 Connect Energy Services, Inc.ENAliquids agreement NGL Supply, Inc. (including PremierENA/EGLIphysical & financial transactions referenced Energy Partners, a division of NGL Supply, Inc.) Plains Marketing, L.P.ERACdeal no. QG4563.1 Plains Marketing, L.P.ERACdeal no. QG4482.2 Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-01-31 12:50:00] Kim S. Theriot:\\nAttached are the Daily Lists for January 29 and January 30 as well as the Master Termination Log, which incorporates all terminations received through January 30. Also, prepetition mutual terminations have been added to this list. They are identified under \"Nature of Default\" as \"mutual termination\". Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-02-05 15:03:35] Kim S. Theriot:\\nNote to Stephanie Panus.... Stephanie...please remove my name as well as Melissa Murphy\\'s from the distribution list below. Please add the following: Todd D. Hall Kevin Sweeney Rita Wynne Rebecca Grace Rhonda Robinson Kerri Thomspon Kristin Albrecht Tom Chapman Thanks! Kim Theriot Attached is the Daily List for January 31 as well as the Master Termination Log, which incorporates all terminations received through January 31. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-02-05 15:06:25] Kim S. Theriot:\\nAttached is the Daily List for February 4 as well as the Master Termination Log, which incorporates all termination received through February 4 (with the exception of February 1, which is under legal review and contains all financial transactions). Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490\\n\\n[2002-05-28 07:20:35] Katherine L. Kelly:\\nPlease look into the CNG LDC (Hope Gas) termination 12/1 and the $66 MM settlement offer that is listed on the Letter Log below. Let me know what that is after you figure it out. If you have any questions, please ask. Ed Attached is the Daily List for May 24, 2002 as well as the Master Termination Log, which incorporates all terminations received through May 24. Stephanie Panus Enron Wholesale Services ph: 713.345.3249 fax: 713.646.3490', 'summary': \"The email thread discusses the Master Termination Log and the need to investigate a CNG LDC (Hope Gas) termination and a $66 million settlement offer. Stephanie Panus sends out the Daily List and Master Termination Log for various dates. Kim Theriot requests her name and Melissa Murphy's name to be removed from the distribution list and adds several names to it. The thread also includes updates on terminations and valid terminations for various companies.\"}\n"
     ]
    }
   ],
   "source": [
    "preprocessor = EmailParquetPreprocessor()\n",
    "\n",
    "# Load parquet file\n",
    "preprocessor.load_parquet('train-00000-of-00001-41f2ca6bce8b68f8.parquet')\n",
    "\n",
    "# Process all threads\n",
    "processed_threads = preprocessor.process_all_threads()\n",
    "\n",
    "# Save processed results\n",
    "preprocessor.save_processed_threads('processed_threads.parquet')\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample processed thread:\")\n",
    "sample_thread = processed_threads[0]\n",
    "print(f\"Subject: {sample_thread['subject']}\")\n",
    "print(f\"Number of messages: {len(sample_thread['messages'])}\")\n",
    "print(\"\\nSample summary input:\")\n",
    "print(sample_thread['summary_input'][:500] + \"...\")\n",
    "print(sample_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cf866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training T5 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97754791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 15:52:02.919640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732654322.930716  888056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732654322.934073  888056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-26 15:52:02.945913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cf1c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3036356a884448994d39e641ad81d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a7af5720e54930863b3c815d890b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 15:54:18,049] A new study created in memory with name: no-name-31f879bb-2ffd-41cf-b7f2-22154e154ce7\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 46:38, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.412594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.426700</td>\n",
       "      <td>0.398668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.393433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.392889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.394116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.397161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.399464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 16:41:25,485] Trial 0 finished with value: 0.3928888440132141 and parameters: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with value: 0.3928888440132141\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [561/561 18:45, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.395483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.407934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 17:00:37,179] Trial 1 finished with value: 0.3954828977584839 and parameters: {'learning_rate': 0.0003963215921716496, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with value: 0.3954828977584839\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [875/875 41:08, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.289300</td>\n",
       "      <td>0.405421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.408773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.411718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.413278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.406888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.406401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.276200</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 17:42:11,585] Trial 2 finished with value: 0.40542060136795044 and parameters: {'learning_rate': 2.211194893748122e-05, 'num_train_epochs': 7, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 3}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with value: 0.40542060136795044\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 25:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.418903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.408722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.420461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 18:08:20,797] Trial 3 finished with value: 0.4087224006652832 and parameters: {'learning_rate': 0.00012183128994501128, 'num_train_epochs': 4, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 2}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished with value: 0.4087224006652832\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 31:05, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.481325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.427186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.430528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 18:39:51,990] Trial 4 finished with value: 0.42718592286109924 and parameters: {'learning_rate': 7.674344902522101e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished with value: 0.42718592286109924\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='276' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [276/276 34:21, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.438869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.440213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.446887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.450695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.440811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.441008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 19:14:43,045] Trial 5 finished with value: 0.438868910074234 and parameters: {'learning_rate': 0.0001067547296540278, 'num_train_epochs': 6, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 finished with value: 0.438868910074234\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 34:22, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.456477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.446983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.454556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>0.456979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.450185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.450723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 19:49:34,877] Trial 6 finished with value: 0.44698265194892883 and parameters: {'learning_rate': 0.00016219787015641065, 'num_train_epochs': 6, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 7}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 finished with value: 0.44698265194892883\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [856/856 49:06, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.525754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.490370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.454157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.458008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.472858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.483319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.495449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 20:39:08,929] Trial 7 finished with value: 0.45415690541267395 and parameters: {'learning_rate': 0.00038600292986741905, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 7}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 finished with value: 0.45415690541267395\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 1:00:52, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.560751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.556746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.552974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.481085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.478580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.478172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.478264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.478293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 21:40:28,950] Trial 8 finished with value: 0.47789615392684937 and parameters: {'learning_rate': 2.778434446999055e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 0.3928888440132141.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 finished with value: 0.47789615392684937\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 23:13, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.521036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.511466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.505250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.510171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 22:04:08,939] Trial 9 finished with value: 0.5052496194839478 and parameters: {'learning_rate': 0.0001731563404182816, 'num_train_epochs': 4, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.3928888440132141.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 finished with value: 0.5052496194839478\n",
      "Best value so far: 0.3928888440132141\n",
      "Best parameters so far: {'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n",
      "{'learning_rate': 0.0003271920187045571, 'num_train_epochs': 8, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_T5_model/tokenizer_config.json',\n",
       " './saved_T5_model/special_tokens_map.json',\n",
       " './saved_T5_model/spiece.model',\n",
       " './saved_T5_model/added_tokens.json',\n",
       " './saved_T5_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "# Load the processed data\n",
    "df = pd.read_parquet('processed_threads.parquet')\n",
    "\n",
    "# Check for NaN values and drop them\n",
    "df = df.dropna(subset=['summary_input', 'summary'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['summary_input', 'summary']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['summary_input', 'summary']])\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the inputs and labels\n",
    "def preprocess_function(threads):\n",
    "    inputs = [thread for thread in threads['summary_input']]\n",
    "    targets = [thread for thread in threads['summary']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=8)\n",
    "val_tokenized_dataset = val_dataset.map(preprocess_function, batched=True, num_proc=8)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    #  hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 10)\n",
    "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8])\n",
    "    gradient_accumulation_steps = trial.suggest_int('gradient_accumulation_steps', 1, 8)\n",
    "\n",
    "    # Define training arguments with suggested hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        report_to='none',\n",
    "        fp16=True, \n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        dataloader_num_workers=8,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized_dataset,\n",
    "        eval_dataset=val_tokenized_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results['eval_loss']\n",
    "\n",
    "# To print information after each trial\n",
    "def print_callback(study, trial):\n",
    "    print(f\"Trial {trial.number} finished with value: {trial.value}\")\n",
    "    print(f\"Best value so far: {study.best_value}\")\n",
    "    print(f\"Best parameters so far: {study.best_params}\")\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10, callbacks=[print_callback])\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)\n",
    "\n",
    "# Save the best model and tokenizer\n",
    "model.save_pretrained('./saved_T5_model')\n",
    "tokenizer.save_pretrained('./saved_T5_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33417ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating summaries for test data using fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125c7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 417 email threads from parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing threads: 100%|██████████| 417/417 [00:00<00:00, 1916.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 417 processed threads to processed_test_threads.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test parquet file\n",
    "preprocessor.load_parquet('test-00000-of-00001-cd15b40aacd3c33e.parquet')\n",
    "\n",
    "processed_test_threads = preprocessor.process_all_threads()\n",
    "\n",
    "# Save processed results\n",
    "preprocessor.save_processed_threads('processed_test_threads.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e49e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 11:18:37.113184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732724317.125239  253602 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732724317.128840  253602 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-27 11:18:37.144566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████| 53/53 [1:15:45<00:00, 85.76s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./saved_T5_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./saved_T5_model')\n",
    "\n",
    "# Load the processed data\n",
    "test_df = pd.read_parquet('processed_test_threads.parquet')\n",
    "\n",
    "# Check for NaN values and drop them\n",
    "test_df = test_df.dropna(subset=['summary_input'])\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Function to split text into emails\n",
    "def split_emails(text):\n",
    "    return text.split('\\n\\n')\n",
    "\n",
    "# Batch processing function\n",
    "def generate_batch_summaries(batch):\n",
    "    summaries = []\n",
    "    for thread in batch:\n",
    "        emails = split_emails(thread['summary_input'])\n",
    "        email_summaries = []\n",
    "        for email in emails:\n",
    "            inputs = tokenizer(email, return_tensors='pt', max_length=512, truncation=True, padding='max_length').to(device)\n",
    "            with autocast():\n",
    "                summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=2, early_stopping=True)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            email_summaries.append(summary)\n",
    "        concatenated_summaries = ' '.join(email_summaries)\n",
    "        \n",
    "        # Generate final summary from concatenated summaries\n",
    "        inputs = tokenizer(concatenated_summaries, return_tensors='pt', max_length=1024, truncation=True, padding='max_length').to(device)\n",
    "        with autocast():\n",
    "            final_summary_ids = model.generate(inputs['input_ids'], max_length=512, min_length=40, length_penalty=2.0, num_beams=2, early_stopping=True)\n",
    "        final_summary = tokenizer.decode(final_summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        summaries.append(final_summary)\n",
    "    return summaries\n",
    "\n",
    "batch_size = 8\n",
    "tqdm.pandas()\n",
    "test_df['generated_summary'] = test_df.groupby(test_df.index // batch_size).progress_apply(lambda x: generate_batch_summaries(x.to_dict('records'))).explode().reset_index(drop=True)\n",
    "\n",
    "# Save the generated summaries\n",
    "test_df.to_csv('t5_generated_test_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3dcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cf3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd4b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email thread discusses preparations for a meeting in Colorado Springs on November 30th. The meeting will take place in the Gaylord Board Room, starting at 8:00am and concluding at 3:00pm. The Broadmoor Transportation Service provides convenient airport transportation options. The email also mentions the availability of Continental Breakfast on Thursday and Friday. The sender provides the top priorities and budget for the Rates/Regulatory group, while also providing Richard Tabor's estimate of how much would be spent in each region.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./saved_T5_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./saved_T5_model')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def summarize_input(input_data, input_type='pdf'):\n",
    "    # Function to read PDF and extract text\n",
    "    def read_pdf(file_path):\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "\n",
    "    # Function to clean text\n",
    "    def clean_text(text):\n",
    "        # Remove forward tags and other unwanted patterns\n",
    "        text = re.sub(r'Fwd:|Forwarded message:', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "        return text\n",
    "\n",
    "    # Read and clean the input content\n",
    "    if input_type == 'pdf':\n",
    "        input_text = read_pdf(input_data)\n",
    "    elif input_type == 'text':\n",
    "        input_text = input_data\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type. Use 'pdf' or 'text'.\")\n",
    "\n",
    "    cleaned_text = clean_text(input_text)\n",
    "\n",
    "    # Tokenize and generate summary\n",
    "    inputs = tokenizer(cleaned_text, return_tensors='pt', max_length=1024, truncation=True, padding='max_length').to(device)\n",
    "    with autocast():\n",
    "        summary_ids = model.generate(inputs['input_ids'], max_length=512, min_length=40, length_penalty=2.0, num_beams=2, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "pdf_summary = summarize_input('Email_thread.pdf', input_type='pdf')\n",
    "print(pdf_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8120aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''Subject: Co Op City\n",
    "\n",
    "[2001-01-29 07:42:00] Kay Mann:\n",
    "When you get a minute I need to discuss the last attachment for the Co Op term sheet (the asset management one). Here's the current state of the draft: We do not have a current CA with PB. I will have one in the am to send out. Do you want Jude, et al to look at the EPC contract stuff? Kay\n",
    "\n",
    "[2001-01-29 10:53:00] Kay Mann:\n",
    "Lisa, I've made some preliminary suggestions on the term sheet. We will have some comments on the asset management attachment, but I don't have those together yet. In looking through our files I don't find a confidentiality agreement. Therefore, I'm attaching a draft of what we would like to have executed so that we can send you other documents. Thanks, Kay\n",
    "\n",
    "[2001-01-29 10:56:00] Kay Mann:\n",
    "---------------------- Forwarded by Kay Mann/Corp/Enron on 01/29/2001 06:55 PM --------------------------- Kay Mann 01/29/2001 06:53 PM cc: Mark Bernstein/HOU/ECT@ECT, Jude Rolfes/ENRON_DEVELOPMENT@ENRON_DEVELOPMENT Lisa, I've made some preliminary suggestions on the term sheet. We will have some comments on the asset management attachment, but I don't have those together yet. In looking through our files I don't find a confidentiality agreement. Therefore, I'm attaching a draft of what we would like to have executed so that we can send you other documents. Thanks, Kay\n",
    "\n",
    "[2001-02-07 09:47:00] Kay Mann:\n",
    "Hi there, I'm forwarding a draft EPC contract for your review. This form of contract assumes that the equipment will be procurred by the Owner, with the understanding that we have not resolved the issue of the timing of the procurement of the equipment, and how it will be procurred. This is intended to be a discussion draft, and is subject to further Enron review and comment. Thanks, Kay\n",
    "\n",
    "[2001-02-07 09:52:00] Kay Mann:\n",
    "---------------------- Forwarded by Kay Mann/Corp/Enron on 02/07/2001 05:52 PM --------------------------- Kay Mann 02/07/2001 05:47 PM cc: Mark Bernstein/HOU/ECT@ECT Hi there, I'm forwarding a draft EPC contract for your review. This form of contract assumes that the equipment will be procurred by the Owner, with the understanding that we have not resolved the issue of the timing of the procurement of the equipment, and how it will be procurred. This is intended to be a discussion draft, and is subject to further Enron review and comment. Thanks, Kay\n",
    "\n",
    "[2001-02-08 06:54:00] Kay Mann:\n",
    "Pardon the delay, but I inadvertently left you off the email. Kay ---------------------- Forwarded by Kay Mann/Corp/Enron on 02/08/2001 02:53 PM --------------------------- Kay Mann 02/07/2001 05:47 PM cc: Mark Bernstein/HOU/ECT@ECT Hi there, I'm forwarding a draft EPC contract for your review. This form of contract assumes that the equipment will be procurred by the Owner, with the understanding that we have not resolved the issue of the timing of the procurement of the equipment, and how it will be procurred. This is intended to be a discussion draft, and is subject to further Enron review and comment. Thanks, Kay\n",
    "\n",
    "[2001-02-13 01:43:00] Kay Mann:\n",
    "Hi Randy, Here's what you missed. My personal preference is to not spend the money to check out the licensing issues at this point since I don't have the outside counsel doing anything now. When John called me last week and asked the question I gave him that opinion, and I had assumed that it had been communicated to you. Sorry about that! Kay ---------------------- Forwarded by Kay Mann/Corp/Enron on 02/13/2001 09:40 AM --------------------------- Kay Mann 02/07/2001 05:47 PM cc: Mark Bernstein/HOU/ECT@ECT Hi there, I'm forwarding a draft EPC contract for your review. This form of contract assumes that the equipment will be procurred by the Owner, with the understanding that we have not resolved the issue of the timing of the procurement of the equipment, and how it will be procurred. This is intended to be a discussion draft, and is subject to further Enron review and comment. Thanks, Kay'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22e80989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kay is forwarding a draft EPC contract to Randy for review. The contract assumes that the equipment will be procured by the Owner, with the exception of a confidentiality agreement. Kay apologizes for leaving Randy off the email. She also mentions that they do not have a current CA with PB and asks if Jude, et al should review the EPC contract.\n"
     ]
    }
   ],
   "source": [
    "text_summary = summarize_input(sample_text, input_type='text')\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6dbda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b913946-815c-4248-a4f9-1ce8298b9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating summaries for test data using base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a356553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [21:48<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load the processed data\n",
    "test_df = pd.read_parquet('processed_test_threads.parquet')\n",
    "\n",
    "# Check for NaN values and drop them\n",
    "test_df = test_df.dropna(subset=['summary_input'])\n",
    "\n",
    "\n",
    "def generate_summary(text):\n",
    "    # Preprocess the text\n",
    "    input_text = \"summarize: \" + text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Generate summaries\n",
    "tqdm.pandas()\n",
    "test_df['generated_summary'] = test_df['summary_input'].progress_apply(generate_summary)\n",
    "\n",
    "test_df.to_csv('t5_base_test_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b387b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e0719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
